# Multimodal Embedding Benchmark Report

## üìä Executive Summary

This report presents a comprehensive performance comparison between **CLIP (multi2vec-clip)** and **OpenAI (text-embedding-3-small)** for multimodal retrieval tasks.

### Key Findings

üèÜ **Overall Winner: OpenAI** (for accuracy)  
‚ö° **Speed Winner: CLIP** (40x faster)

---

## üéØ Benchmark Setup

### Dataset
- **Size**: 50 items
- **Categories**: 5 (animals, vehicles, nature, food, architecture)
- **Distribution**: 10 items per category
- **Format**: Synthetic images (400x300 PNG) with text descriptions
- **Ground Truth Queries**: 10 queries with known relevant documents

### Evaluation Metrics

1. **Precision@K**: Fraction of retrieved items that are relevant
2. **Recall@K**: Fraction of relevant items that are retrieved
3. **Mean Average Precision (mAP)**: Average precision across all queries
4. **Hit Rate@K**: Percentage of queries with at least one relevant result
5. **Query Time**: Average time per query in milliseconds

### K Values Tested
- K = 1, 3, 5, 10

---

## üìà Results Comparison

### Precision@K

| K | CLIP | OpenAI | Winner | Improvement |
|---|------|--------|--------|-------------|
| 1 | 0.2000 | **0.3000** | OpenAI | +50% |
| 3 | 0.4667 | **0.6333** | OpenAI | +36% |
| 5 | 0.5400 | **0.7000** | OpenAI | +30% |
| 10 | 0.5100 | **0.7200** | OpenAI | +41% |

**Analysis**: OpenAI consistently outperforms CLIP across all K values, showing 30-50% better precision. This indicates OpenAI's text embeddings are better at ranking truly relevant results higher.

### Recall@K

| K | CLIP | OpenAI | Winner | Improvement |
|---|------|--------|--------|-------------|
| 1 | 0.0200 | **0.0300** | OpenAI | +50% |
| 3 | 0.1489 | **0.2011** | OpenAI | +35% |
| 5 | 0.2878 | **0.3722** | OpenAI | +29% |
| 10 | 0.5456 | **0.7667** | OpenAI | +41% |

**Analysis**: OpenAI demonstrates superior recall, retrieving 29-50% more relevant documents. At K=10, OpenAI retrieves 76.67% of all relevant documents versus CLIP's 54.56%.

### Hit Rate@K

| K | CLIP | OpenAI | Winner |
|---|------|--------|--------|
| 1 | 0.2000 | **0.3000** | OpenAI |
| 3 | 0.8000 | 0.8000 | Tie |
| 5 | 0.8000 | 0.8000 | Tie |
| 10 | 0.8000 | 0.8000 | Tie |

**Analysis**: Both models achieve 80% hit rate at K‚â•3, meaning 8 out of 10 queries return at least one relevant result. OpenAI has a 50% better hit rate at K=1.

### Mean Average Precision (mAP)

| Model | mAP | Winner |
|-------|-----|--------|
| CLIP | 0.6822 | - |
| OpenAI | **0.8572** | OpenAI (+25.7%) |

**Analysis**: OpenAI's mAP is 25.7% higher, indicating better overall ranking quality across all queries.

### Query Performance

| Model | Avg Query Time | Winner |
|-------|----------------|--------|
| CLIP | **26.50 ms** | CLIP |
| OpenAI | 1061.08 ms | - |

**Analysis**: CLIP is approximately **40x faster** than OpenAI, with average query times of 26.5ms vs 1061ms. This is because:
- CLIP runs locally (no API calls)
- OpenAI requires network round-trip to their API
- OpenAI has rate limiting and processing overhead

---

## üîç Detailed Analysis

### Why OpenAI Performs Better on Accuracy

1. **Advanced Language Model**: text-embedding-3-small benefits from OpenAI's large-scale pretraining on diverse text data
2. **Semantic Understanding**: Better at capturing nuanced semantic relationships in text descriptions
3. **Text-Optimized**: Our synthetic dataset is text-description based, playing to OpenAI's strengths

### Why CLIP is Faster

1. **Local Inference**: Runs entirely on your machine, no network latency
2. **Optimized Architecture**: Designed for efficient multimodal embedding
3. **No Rate Limits**: Can process unlimited queries without API constraints

### When to Use Each Model

#### Use CLIP When:
- ‚úÖ Low latency is critical (real-time applications)
- ‚úÖ High query volume expected
- ‚úÖ You have actual images (not just text descriptions)
- ‚úÖ Want to avoid API costs
- ‚úÖ Need offline capabilities
- ‚úÖ Privacy is important (data stays local)

#### Use OpenAI When:
- ‚úÖ Accuracy is paramount
- ‚úÖ Query volume is moderate
- ‚úÖ Working primarily with text
- ‚úÖ Can tolerate ~1 second query latency
- ‚úÖ Have budget for API calls
- ‚úÖ Need best-in-class text understanding

---

## üí∞ Cost Analysis

### CLIP
- **Setup Cost**: Free (open source)
- **Runtime Cost**: Free (local compute)
- **Infrastructure**: Requires GPU for optimal performance (~2GB VRAM)
- **Scalability**: Limited by local hardware

### OpenAI
- **Setup Cost**: Free
- **Runtime Cost**: $0.00002 per 1K tokens
- **For 1M queries** (avg 50 tokens each): ~$1.00
- **Infrastructure**: None (managed by OpenAI)
- **Scalability**: Unlimited (subject to rate limits)

**Example Cost Calculation**:
```
50,000 queries/day √ó 30 days = 1.5M queries/month
1.5M queries √ó 50 tokens/query = 75M tokens
75M tokens / 1000 √ó $0.00002 = $1.50/month
```

---

## üé® Dataset Characteristics

### Image Distribution
```
animals:      10 items (20%)
vehicles:     10 items (20%)
nature:       10 items (20%)
food:         10 items (20%)
architecture: 10 items (20%)
```

### Sample Descriptions
- "A golden dog playing in a park"
- "Red car parked on the street"
- "Beautiful mountain landscape"
- "Delicious pizza on a plate"
- "Modern building in the city"

### Ground Truth Queries
- Category-specific queries (matching same category items)
- Generic cross-category queries
- Total: 10 queries with known relevant items

---

## üî¨ Methodology

### Data Collection
1. Generated 50 synthetic images using PIL (400x300 resolution)
2. Created descriptive text for each image
3. Organized into 5 balanced categories
4. Generated ground truth query set with relevance labels

### Embedding Process
1. **CLIP**: Both image and text embedded using multi2vec-clip
2. **OpenAI**: Text-only embedded using text-embedding-3-small

### Evaluation Process
1. For each ground truth query:
   - Retrieve top-K similar items
   - Compare with known relevant items
   - Calculate metrics (Precision, Recall, Hit Rate)
2. Average metrics across all queries
3. Calculate mAP using interpolated precision-recall

---

## üìä Benchmark Results (Raw Data)

### CLIP Results
```json
{
  "precision_at_k": {
    "1": 0.2000,
    "3": 0.4667,
    "5": 0.5400,
    "10": 0.5100
  },
  "recall_at_k": {
    "1": 0.0200,
    "3": 0.1489,
    "5": 0.2878,
    "10": 0.5456
  },
  "hit_rate_at_k": {
    "1": 0.2000,
    "3": 0.8000,
    "5": 0.8000,
    "10": 0.8000
  },
  "map": 0.6822,
  "avg_query_time": 0.02650,
  "total_queries": 10
}
```

### OpenAI Results
```json
{
  "precision_at_k": {
    "1": 0.3000,
    "3": 0.6333,
    "5": 0.7000,
    "10": 0.7200
  },
  "recall_at_k": {
    "1": 0.0300,
    "3": 0.2011,
    "5": 0.3722,
    "10": 0.7667
  },
  "hit_rate_at_k": {
    "1": 0.3000,
    "3": 0.8000,
    "5": 0.8000,
    "10": 0.8000
  },
  "map": 0.8572,
  "avg_query_time": 1.06108,
  "total_queries": 10
}
```

---

## üéØ Recommendations

### For Production Systems

1. **Hybrid Approach**: Use CLIP for initial fast retrieval, OpenAI for re-ranking top results
   - Get CLIP's speed for broad search
   - Use OpenAI's accuracy for final ranking
   - Best of both worlds

2. **Caching Strategy**: Cache OpenAI embeddings to reduce API calls
   - Embed documents once, store embeddings
   - Query embeddings are the only runtime cost
   - Significantly reduces costs

3. **Adaptive Selection**: Choose model based on query characteristics
   - CLIP for image-heavy queries
   - OpenAI for text-heavy queries
   - Route intelligently

### Optimization Tips

#### For CLIP:
- Use GPU acceleration
- Batch process when possible
- Pre-compute embeddings offline
- Consider fine-tuning on your domain

#### For OpenAI:
- Implement response caching
- Use batch API when available
- Monitor rate limits
- Consider dedicated capacity for high volume

---

## üìù Reproducibility

### Files Created
```
generate_synthetic_dataset.py  - Dataset generator
benchmark_multimodal.py        - Benchmark runner
synthetic_data/
  ‚îú‚îÄ‚îÄ synthetic_dataset.csv    - Dataset metadata
  ‚îú‚îÄ‚îÄ ground_truth_queries.csv - Evaluation queries
  ‚îî‚îÄ‚îÄ image_*.png              - 50 synthetic images
benchmark_results.json         - Raw results
BENCHMARK_REPORT.md           - This report
```

### To Reproduce
```bash
# Generate dataset
python generate_synthetic_dataset.py

# Run benchmark
python benchmark_multimodal.py

# Results saved to benchmark_results.json
```

### To Use Custom Dataset
```python
# Your CSV should have columns:
# - id: unique identifier
# - description: text description
# - category: category label
# - image_path: path to image file
# - filename: image filename

# Modify dataset path in benchmark_multimodal.py:
dataset_csv = "your_custom_dataset.csv"
```

---

## üöÄ Future Work

### Potential Improvements

1. **Larger Dataset**: Test with 1000+ items for more statistical significance
2. **Real Images**: Use actual photos instead of synthetic images
3. **More Categories**: Expand beyond 5 categories
4. **Fine-tuning**: Fine-tune CLIP on domain-specific data
5. **Hybrid Models**: Test ensemble approaches
6. **Different K Values**: Test K=20, 50, 100
7. **User Study**: Validate with human relevance judgments

### Additional Metrics

1. **NDCG (Normalized Discounted Cumulative Gain)**: Ranking quality with position weights
2. **MRR (Mean Reciprocal Rank)**: Position of first relevant result
3. **F1@K**: Harmonic mean of precision and recall
4. **Diversity**: Measure result diversity
5. **Novelty**: How unique are the results

---

## üìû Conclusion

Both CLIP and OpenAI embeddings have their strengths:

**CLIP** excels at:
- Speed (40x faster)
- Cost (free)
- Privacy (local)
- True multimodal (image+text)

**OpenAI** excels at:
- Accuracy (+25-50% on metrics)
- Text understanding
- Semantic quality
- Ease of use

**Recommendation**: For most production use cases, consider a **hybrid approach** that leverages CLIP's speed for broad retrieval and OpenAI's accuracy for final ranking. This provides the best balance of performance, cost, and quality.

---

**Report Generated**: November 4, 2025  
**Benchmark Version**: 1.0  
**Dataset**: Synthetic (50 items, 5 categories)  
**Queries**: 10 ground truth queries


